Design for the C code
=====================

Let's do recursive descent first.  It's simple and we can try to limit
the stack manually (currently CPython only allows about 100 nested
parentheses so that's okay).

Parser structure
----------------

Also needs to point to a PyArena, used for allocating AST nodes and
other things.  And maybe some flag indicating there's an allocation
error and a convention to bail if there is one.

Possibly return Py_None for "no match" rather than NULL?  That will
allow NULL to mean "error" (other than syntax error or EOF).  But it
will be very clumsy, make all generated code more complex (may have to
introduce goto).

- tok: Pointer to tokenizer, CPython's struct tok_state
- input: Pointer to input bytes (char *), same as tok->input (owned by tok).
         (Not used though.  And tokenizing from file won't have it.)
- tokens: Pointer to array of Token structs
- mark: index into array of Tokens
- fill: number of valid entries in array of Tokens
- size: total number of entries in array of Tokens
- arena: memory allocation arena (owns all AST, Token, Memo structures allocated)

Token structure
---------------

These are in an array linked from Parser.  (Or linked list???)

- type: int, token type (needs only 8 bits)
- value: bytes object [or maybe just two indices into parser->input?)
- line, col, endline, endcol: int
- memo: Pointer to linked list Memo

Memo structure
--------------

Linked list, optimized for quickly finding a given type.  (Or array???)

- type: int, either a token or a rule (rules start at 256)
- node: NULL or pointer to AST node OR pointer to token object (TODO)
- mark: if node != NULL, index into Parser's array of tokens
- next: NULL or pointer to next Memo structure

C function for a rule
---------------------

Similar to the Python functions, but memoziation is done here.

// Constants for rules: expr_type, term_type, etc.
// Constants for tokens: NAME, NUMBER, NEWLINE, LPAR, RPAR, etc.
// Functions for rules: expr_rule, term_rule, etc.

#define expr_type  321

static AST*
expr_rule(Parser *p)
{
    AST *res = NULL;
    int mark = p->mark;
    if (is_memoized(p, expr_type, res))
        return res;
    // Alternatives start here
    if ((a = rule_a(p)) && (b = rule_b(p)) && (c = rule_c(p))) {
        // On success
        res = <make new AST node from (a, b, c)>
        insert_memo(p, mark, expr_type, res);
        return res;
    }
    p->mark = mark;
    // More alternatives...
    ...
    // At the end
    // Memoize negative result too!
    insert_memo(p, mark, expr_type, NULL);
    return NULL;
}

// Here, mark is the start of the node, while p->mark is the end.
// If node==NULL, they should be the same.
static void
insert_memo(Parser *p, int mark, int type, AST *node)
{
    // Insert in front
    Memo *m = PyArena_Malloc(p->arena, sizeof(Memo));
    if (m == NULL)
        panic();  // TODO: How to handle malloc failures
    m->type = type;
    m->node = node;
    m->mark = p->mark;
    m->next = p->tokens[mark].memo;
    p->tokens[mark].memo = m;
}

static int  // bool
is_memoized(Parser *p, int type, AST *pres)
{
    Token *t = &p->tokens[p->mark];
    Memo *m;
    for (m = t->memo; m != NULL; m = m->next) {
        if (m->type == type) {
            if (m->node == NULL)
                return 0;
            p->mark = m->mark;
            *pres = m->node;
            return 1;
        }
    }
    return 0;
}

Structure for each rule
-----------------------

AST *a, *b, *c;
if ((a = a_rule(p) && (b = b_rule(p)) && (c = c_rule(p))) {
    AST *res = SomeAstRule(a, b, c, p->arena);  // Often also add line, col, endline, endcol
    if (res == NULL)
        panic();
    insert_memo(p, mark, expr_type, res);
    return res;
}
p->mark = mark;  // Prep for the next alternative, or for fail return

Expecting tokens
----------------

(This is where the token position actually gets moved.)

NUMBER: return Constant(parsenumber(c, string), NULL, line, col, endline, endcol, p->arena)
STRING: return <something similar but more complicated>
ELLIPSIS: return Constant(Py_Ellipsis, NULL, line, col, endline, endcol, p->arena)
NAME: return Name(id, Load/Store, line, col endline, endcol, p->arena)
      But if it's a keyword, something else?
OP: Translate into something else?  Does it always mean an error?
    Only seems to be generated for things like ? or $.
LPAR, 'if', etc.: return something appropriate
      (a Python object added with PyArena_AddPyObject(p->arena, obj),
      or a special marker object

Conclusion, we may need a way to fit tokens (both with extra value
like NUMBER and without, like LPAR) in the return types of grammar
rule functions, unless we want to use a different type for expect()
kinds of things.

Let's say we have (memoized) expect(NAME) in the Python version; in
the C version we'd have

static AST*
expect_name(Parser *p)
{
    AST *res;
    if (is_memoized(p, NAME, &res))
        return res;
    int mark = p->mark;
    Token *t = next_token(p);  // effectively &p->tokens[p->mark++]
    if (t->type == NAME) {
        // TODO: Load/Store distinction
        res = Name(t->string, Load, <line/col info>, p->arena);
        insert_memo(p, mark, NAME, res);
        return res;
    }
    insert_memo(p, mark, NAME, NULL);
    return NULL;
}

And ditto for STRING

OTOH for all tokens without extra info we could have one function:

static AST*
expect_token(Parser *p, int type)
{
    AST *res;
    if (is_memoized(p, NAME, &res))
        return res;
    int mark = p->mark;
    Token *t = next_token(p);  // effectively &p->tokens[p->mark++]
    if (t->type == type) {
        res = <some constant>  // TODO
        insert_memo(p, mark, type, res);
        return res;
    }
    insert_memo(p, mark, type, NULL);
    return NULL;
}

What should the result type be here?  Something that's a subclass of
AST.  For things like LPAR, there's no precedent.  Also, maybe this
shouldn't bother with memoization, since the info is all there.  Just
inline the fast path of next_token(p).
